---
title: "LRM Assignment 2"
author: '232807001'
date: "2023-12-04"
output: html_document
---

## Read in dataset

```{r}
library(readxl)
df=read_excel("REG1.xlsx")
nrow(df)
ncol(df)
```

## About the data

-   A1, Petrol tax (cents per gallon)

-   A2, Average income (dollars)

-   A3, Paved Highways (miles)

-   A4, Proportion of population with driver's licenses

-   B, Consumption of petrol (millions of gallons)

## Descriptives

```{r}
library(psych)
describe(df)
```

```{r}
attach(df)
summary(A1)
summary(A3)
```

```{r}
shapiro.test(A1)
shapiro.test(A2)
shapiro.test(A3)
shapiro.test(A4)
```

### Descriptives for normally distributed variables

|     | n   | Mean    | SD     |
|-----|-----|---------|--------|
| A4  | 48  | 0.57    | 0.06   |
| A2  | 48  | 4241.83 | 573.62 |

### Descriptives for non-normally distributed variables

|     | n   | min    | Q1   | Q2   | Q3    | max      |
|-----|-----|--------|------|------|-------|----------|
| A1  | 48  | 5.00   | 7.00 | 7.5  | 7.668 | 10.00    |
| A3  | 48  | 431.00 | 3110 | 4736 | 7156  | 17782.00 |

## Fit model 1

```{r}
model1 = lm(B~.,data=df)
summary(model1)
```

$$
\widehat{y} = \widehat{\beta_{0}} + \widehat{\beta_{1}}\space x_{1} + \widehat{\beta_{2}}\space x_{2} + \widehat{\beta_{3}}\space x_{3} + \widehat{\beta_{4}}\space x_{4} \\
where \space \beta_{0} = intercept \\
x_{1} = petrol\space tax(cents\space per\space gallon) \\
x_{2} = average\space income(dollars) \\
x_{3} = proportion\space of\space population\space with\space their\space driver's\space licenses \\ 
x_{4} = paved highways (in miles) \\ 
y = consumption\space of\space petrol (millions\space of\space gallons)
$$

```{r}
plot(model1)
```

### Influential Observations

```{r}
cooksD = cooks.distance(model1)
influential <- cooksD[(cooksD > (3 * mean(cooksD,na.rm=TRUE)))]
influential

names_of_influential = names(influential)
names_of_influential

plot(model1,4)
```

Since all points have cooks distance value \< 1, there are no influential observations in the model.

### Remove outliers through boxplot

```{r}
boxplot(df$B)$out
df$B
```

```{r}
df1 = df[df["B"]!= 865,]
df1 = df1[df1["B"]!=968,]
print(boxplot(df1$B)$out)
```

## Level of Signifance

we use 0.05 level of significance for tests of parameters of regression.

## Fit model 2

```{r}
model2 = lm(B~.,data=df1)
summary(model2)
summary(model1)
```

Seems model1 has better R-square value, lesser residual standard error (meaning better prediction of petrol consumption).

Therefore we will proceed with model1 and check assumptions required for linear regression.

### Checking assumptions

1.  Homoskedasticity

```{r}
plot(fitted(model1),residuals(model1),xlab="Fitted",ylab="Residuals")
abline(h=0)
```

The plot of residuals vs fitted values has no particular pattern in the plot. Therefore , we apply formal tests for testing heteroskedasticity. They are equally distributed around the zero residual line.

Breusch Pagan Test

```{r}
library(lmtest)
bptest(model1)
```

since p-value = 0.1604 \> 0.05, we fail to reject the null hypothesis and conclude that homoskedasticity is present.

2.  Normality Of Residuals

```{r}
plot(model1,2)
qqnorm(df$B, main="Normal Q-Q Plot")
qqline(df$B, col = "red", lwd = 2)
```

Q-Q plot shows that the residuals follows the line. therefore the residuals are approximately normally distributed. We further confirm this with formal test for normality.

```{r}
shapiro.test(residuals(model1))
```

Shapiro-wilk normality test shows that the residuals of model1 are normally distributed since p-value \> 0.05, we fail to reject the null hypothesis.

3.  Autocorrelation

```{r}
require(lmtest)
dwtest(model1)
```

p-value is greater than 0.05 which leads us to fail to reject the null hypothesis and conclude that the errors are not autocorrelated by durbin watson test.

4.  Multicollinearity

```{r}
library(car)
vif(model1)
```

Since VIF values of each regressor is less than 5 , we can conclude there is no multicollinearity present in the model.

5.  Linearity

```{r}
avPlots(model1)
```

From scatter plots for each independent variable vs dependent variable, there is linear relationship b/w each of them. We can still there is an extreme outlier in the response variable (40th observation with value 968 - identified using boxplot earlier.). We can try removing it and checking if there is improvement in the model.

### Stepwise Regression

```{r}
intercept_only <- lm(B ~ 1, data=df)
all <- lm(B ~ ., data=df)
both <- step(intercept_only, direction='both', scope=formula(all), trace=0)
both$anova
```

```{r}
model3 = both
summary(model3)
summary(model1)
```

### Model Adequacy

Model 3 obtained through stepwise regression technique of variable selection has lesser residual standard error and adjusted R-squared compared to model1. We will proceed with model3 now.

Model 3 explains about 65.27 % of the variability in regression.

Also the Wald's t-test shows us that A3 is not significantly contributing towards response in model 1, which has been removed in model 3 leading to better significance of A1.

Let us check for residual plots for model 3.

```{r}
plot(model3)
plot(model1)
```

-   Clearly the plots seem to have improved for model 3, since influential points have been reduced (from residual vs leverage plot).

-   Residuals vs fitted values plot show more uniform distribution of points around the zero residual line.

-   However the Q-Q plot show that the points on the right seem to deviate a little from the line. Let us verify normality for model 3 using shapiro wilk normality test.

```{r}
shapiro.test(residuals(model3))
```

The null hypothesis is rejected and hence we conclude residuals are not normally distributed for model 3.

### Interactions

```{r}
library(emmeans)
emmip(model3, A1 ~ A2, cov.reduce=range)
emmip(model3, A1 ~ A4, cov.reduce=range)
emmip(model3, A2 ~ A4, cov.reduce=range)
```

There are no interactions present between regressors. Therefore we wont go for creating new models with any interactions.

The final fitted multiple linear regression model is given as follows :

$$
\widehat{y} = \widehat{\beta_{0}} + \widehat{\beta_{1}}\space x_{1} + \widehat{\beta_{2}}\space x_{2} + \widehat{\beta_{3}}\space x_{3} \\
where \space \beta_{0} = intercept \\
x_{1} = petrol\space tax(cents\space per\space gallon) \\
x_{2} = average\space income(dollars) \\
x_{3} = proportion\space of\space population\space with\space their\space driver's\space licenses \\  
y = consumption\space of\space petrol (millions\space of\space gallons)
$$

```{r}
summary(model3)
```

Interpretation :

1.  Intercept : is not interpretable since petrol tax cannot be made equal to zero and zero proportion of population with driver's license does not make sense.
2.  Petrol tax (cents per gallon) : for one cent per gallon increase in petrol tax , the petrol consumption is expected to decrease by 29.48 million gallons adjusting for other regressors.
3.  Average income (dollars) : for one dollar increase in average income, the petrol consumption is expected to decrease by 0.07 million gallons adjusting for other regressors.
4.  Proportion of population with driving license : for unit increase in this proportion , the petrol consumption is expected to increase by 1374.77 million gallons adjusting for other regressors.

Therefore, it can be concluded that proportion of population with driving license seems cause greater change in petrol consumption in the country. A good solution would be to stop usage of vehicles older than 17 years to reduce pollution caused by outdated and inefficient engine models and promote public transport by increasing their numbers and enabling better connections and routes between various places.

### Prediction using model on train data

```{r}
df1 = data.frame(A1=df$A1,
                 A2=df$A2,
                 A3=df$A3,
                 A4=df$A4)
head(df1)
head(predict(model3,df1),5)
head(df$B,5)
```
